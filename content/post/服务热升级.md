---
title: "数据服务研发指南"
date: 2018-07-10T09:13:30+08:00
draft: true
categories: ["后端架构"]
tags: ["后端架构"]
topics: ["后端架构"]
---

数据服务重启往往有个大问题，数据加载太慢，导致上线部署时间长，进而不敢调整，谨小慎微；服务本身也是能忍就忍，不愿意深度优化；毕竟如果其它手段不够充分，上线异常导致数据错误，回滚也无济于事。

方案一：

新进程启动，旧进程停服务，mmap共享数据（或者今天ipc方式），同步完成之后关掉旧进程，新进程监听端口。类似版本服务，允许多个版本的进程存在，但是只能有一个提供服务。

方案二：

核心功能实现为动态链接库，更新.so文件，主进程卸载旧库，加载新动态链接库。

方案三：

数据加载服务和lbs_index服务分开，常规更新不修改数据加载服务，启动速度最起码能和lbs_proxy一样快。但是很难一开始就分配足够大的共享内存，会降低lbs效率。

1、集成环境搭建lbs_index和lbs_proxy服务。

lbs_proxy发送数据到线上mq，同时lbs_index消费线上mq数据，完整模拟线上环境；lbs修改上线先在集群测试（lbs_index和lbs_proxy分别升级）。

集成环境的交易模块要改成使用集成lbs。

 

2、数据快速恢复和重放。

集成环境部署一个新服务，消费wproxy消息，本地保存orde和route的新增删除，每10分钟切换文件保存消息；消息保存两周。

目前计划写到类似WAL的文件中，两个文件快速切换并后台重命名。

实际上wproxy qps非常小，撑死500，单机10线程记录绰绰有余。

 

3、lbs相关工具

如2所说，要能解析WAL文件并重写mq，让线上机器消费恢复数据；包括revert操作（主要是新增改为删除）、重发数据。这样可以省去API依赖。

lbs_proxy和lbs_index召回测试工具

lbs数据检查工具，能取出某个地理范围或者时间段所有数据


LBS作为后端最重要的数据服务，已经处于“稳态”，很少有需求只能通过修改lbs完成；而它本身的稳定性也值得称赞。但是随着顺风车规模扩大，交易模式创新，lbs又会阻碍业务发展；比较显著的有以下几个问题：

1、召回模式单一，而且跟业务强相关。

现有lbs召回逻辑复杂，跟new_lbs_client、lbs_proxy、index_service、lbs_index等多处代码相关，参数几乎类似却不统一；ddfp_module中封装了大量lbs召回接口，但是在new_lbs_client中还可以看到BywayOrder、AroundOrder等包含业务逻辑的接口，层次不清晰，不易维护，出现问题无从排查。而要新增召回方式，如沿途多点召回，需要业务端做很多工作，在多个模块中新增代码；并且不能保证效率，无法有效截断。

2、启动时间过长

这可能是目前lbs存在的最大问题，lbs_index_order启动时间需要一分半，lbs_index_route则需要3分钟，全部上线需要一个半小时。上线过程中会有大量报警，且出问题时无法快速回滚。

3、工具缺乏

主要包括召回、数据统计、数据校验、数据恢复等。缺乏必要的辅助工具，使得lbs变成了黑盒，只能通过业务代码来判断运行状况。

4、不易维护

lbs_index是有状态服务，通过mq同步数据。大量的消费组，依赖local ip，不方便维护；同时依靠city_id做分片，目前四个分区数据严重不均衡。

 

针对上面四个问题，提出如下解决方案，并将其作为lbs2.0的重点升级内容：

1、提供召回新接口，兼容原有召回接口的同时，提供更多复杂的召回功能

提供lbs_recall接口，并统一参数，上面提到的所有层级都统一使用一个参数，由lbs_index模块的thrift文件维护，无需每层变动，只要透传即可。